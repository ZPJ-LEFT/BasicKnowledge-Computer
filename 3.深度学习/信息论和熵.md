# 信息论和墒

基本概念：发生的事件概率越小，越是能给我们更多的信息量。

## 自信息
对于一个事件集合$X=\{x_{i}|i=0,...,n\}$，每个事件发生的概率是$p(x_i)$，则对于一个事件$x_i$而言，它包含的信息量（自信息量）可以定义为:

$$
I(x) = -log(p(x))
$$

通常情况下取log的底为2。

- 条件自信息

一个事件的发生可能会导致另一事件发生的概率发生变化。已知y事件已经发生，此时受事件y影响的事件x的条件自信息量定义为:

$$
I(x|y) = -log(p(x|y))
$$

- 互信息

互信息衡量的是两个随机变量之间共享了多少信息，或者说，知道一个变量后，对另一个变量有多少“确定性提升”。

$$
\begin{aligned}
I(x;y) & = I(x) - I(x|y)\\
       & = I(y) - I(y|x)
\end{aligned}
$$


# 熵

对于一个离散变量x，它的熵公式可以表示为：

$$
\begin{aligned}
H(x) & = -\sum_{i=1}^{n}P(x)log(P(x)) \\
        & = -E_{x\sim P}(log(P(x)))
\end{aligned}
$$

## 交叉熵
对于概率分布P和概率分布Q，二者的交叉熵公式为：

$$
H(P,Q) = -E_{x\sim P}(log(Q(x)))
$$

$$
H(Q,P) = -E_{x\sim Q}(log(P(x)))
$$

通常而言使用第一个式子，通常用来衡量一个真实分布P和一个预测分布Q之间的差异。

特别的，对于二分类问题，有二分类交叉熵:

$$
\begin{aligned}
BCE(P,Q) & = -P(x_{0})log(Q(x_{0})) - P(x_{1})log(Q(x_{1}))  \\
& = -P(x_{0})log(Q(x_{0})) - (1-P(x_{0}))log(1-Q(x_{0})) \\
& = -Plog(Q) - (1-P)log(1-Q)
\end{aligned}
$$

## KL散度

KL 散度通常用来度量两个分布之间的差异，可以理解为用Q来近似P所损失的信息。

$$
\begin{aligned}
D_{KL}(P||Q) & = H(P,Q) - I(P) \\
             & = E_{x\sim P} log(\frac{P(x)}{Q(x)}) \\
             & = \sum P(x)log(\frac{P(x)}{Q(x)})
\end{aligned}
$$

KL散度不满足交换律，有正向、反向之分。

就优化方向而言，当P(x)分布已知，KL散度就等价于交叉熵。特别的，当P(x)是One-hot编码时，二者在计算上等价。

# 参考文献

- [【信息论】自信息与互信息](https://zhuanlan.zhihu.com/p/523745054)