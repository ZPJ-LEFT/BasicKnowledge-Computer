# 激活函数

## 一、基本概念

### 定义

激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。

### 为什么要引入激活函数
将线性模型转化为非线性模型，如果不引入激活函数，无论神经网络结构多么复杂、层数多么深，网络的输出都是输入的线性组合，无法解决更复杂的问题。


### 分类

- **线性与非线性**：激活函数可以分为线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等）以及非线性激活函数（非线性方程控制输入到输出的映射，比如Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等）
- **饱和与非饱和**：当n趋于正/负无穷，激活函数导数趋近于0，那么我们称之为右/左饱和，如果一个函数既满足左饱和也满足右饱和，称其为饱和函数。饱和激活函数包括Sigmoid,Tanh，非饱和激活函数包括ReLU,LReLU,ELU,PReLU,RReLU

### 性质

- 连续可导（允许少数点上不可导）
- 激活函数及其导数尽量简单，方便网络计算
- 激活函数的导函数值域处于合适的区间，不能过大或过小，避免梯度爆炸/梯度消失

## 二、常见的激活函数

### (1) Sigmoid

也叫Logistic函数，表达式为 $f(x) = \frac{1}{1+e^{-x}}$

常用性质包括：
- 输出范围[0,1]
- 用于将预测概率作为输出的模型
- 梯度平滑，处处可导

存在的不足：
- 梯度消失：输入值越趋近于正负无穷，Sigmoid导数越趋近于0
- 不以0为中心：Sigmoid函数输出恒大于0，非中心化的输出会使下一层神经元输出发生偏置偏移（Bias Shift）
- 计算成本高

### (2) Tanh（双曲正切激活函数）

表达式如下所示：
$$
\begin{aligned}
f(x) & = \frac{e^{x}-e^{-1}}{e^{x}+e^{-1}} \\ 
& = \frac{2}{1+e^{-2x}}+1
\end{aligned}
$$

实际上可以将Tanh视为缩放、平移后的Sigmoid函数，并且其公式等价为 $f(x)= 2sigmoid(2x)+1$

优势：以0为中心

不足：仍存在梯度消失的问题

### (3) ReLU

一种分段线性函数，公式如下：

$$
f(x) = 
\begin{cases}
x,\,\,x>=0 \\
0,\,\,x<0 \\
\end{cases}
$$

优势：
- 当输入为正时，导数恒为1，弥补了Sigmoid和tanh的梯度消失问题
- 求导简单，计算速度快
- 具有生物学依据，如单侧抑制、宽兴奋边界

不足：存在DeadReLU问题，即输入为负数时，ReLU完全失效

### (4) Leaky ReLU

表达式如下，其中$\gamma$是一个很小的数
$$
f(x) = 
\begin{cases}
x,\,\,x>=0 \\
\gamma x,\,\,x<0 \\
\end{cases}
$$

优势：缓解了dead ReLU问题

缺陷：不能完全证明LReLU总是优于ReLU

### (5) Parametric ReLU

$$
f(x) = 
\begin{cases}
x,\,\,x>=0 \\
\gamma_{i} x,\,\,x<0 \\
\end{cases}
$$

### (6) ELU

$$
f(x) = 
\begin{cases}
x,\,\,x>=0 \\
\alpha (e^{x}-1),\,\,x<0 \\
\end{cases}
$$

### (7) SeLU

$$
f(x) = \lambda ELU(x)
$$

### (8) SoftMax

$$
f(x_{i}) = \frac{e^{x_{i}}}{\sum_{j} e^{j}}
$$

### (9) Swish

又称为自门控激活函数，可以视为线性函数和ReLU函数之间的非线性插值函数，其程度由参数$\beta$控制

$$
\sigma (x)=x*sigmoid(\beta x)
$$
